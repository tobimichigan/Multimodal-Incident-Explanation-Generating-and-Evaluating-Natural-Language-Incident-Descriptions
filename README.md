# Multimodal-Incident-Explanation-Generating-and-Evaluating-Natural-Language-Incident-Descriptions
Multimodal Incident Explanation: Generating and Evaluating Natural-Language Incident Descriptions with METEOR, CiDER-D and SPICE for Dashcam Data

Original Article Publication: https://handsonlabs.org/multimodal-incident-explanation-generating-and-evaluating-natural-language-incident-descriptions-with-meteor-cider-d-and-spice-for-dashcam-data/?v=c6a82504ceeb

<h1 align="center">Abstract</h1>

<p>This paper introduces a multimodal framework that couples incident detection with natural-language incident explanation for dashcam footage. The system leverages compact, handcrafted video descriptors to trigger textual caption generation using a rule- and-template-based ImprovedTextGenerator, and evaluates textual fidelity with METEOR, SPICE and a novel CiDER-D metric. We describe dataset curation, model architectures, evaluation protocol and present extensive ablations and qualitative examples. Results show that motion- and edge-informed incident selection improves caption quality, and that CiDER-D better correlates with human judgments for short, structured incident descriptions. We provide reproducible implementation notes and discuss deployment considerations for traffic-safety applications.
</p>

<p>Keywords: Multimodal captioning; incident explanation; CiDER-D; METEOR; SPICE; dashcam analytics; template-based generation </p>

<h1>Research Gaps & Major Contribution to Knowledge</h1>

<p>In the intersection of computer vision, transportation safety, and multimodal AI, significant gaps hinder the development of robust dashcam incident analysis. Most existing approaches operate on a single modality or treat detection and explanation as disjoint steps. In particular, many vision-based traffic systems focus on extracting low-level cues (e.g. trajectories or object detections) but rely on laborious manual post‑processing to derive higher-level insights[1]. As Zhang et al. observe, traditional vision pipelines “require laborious post-processing to derive actionable insights” from traffic video[1], indicating a need for more integrated, end-to-end methods. Likewise, Tami et al. note that conventional ADAS struggle with fragmented sensor processing and advocate multimodal models that integrate visual, spatial, and contextual inputs for holistic scene understanding[2]. However, these models have not yet been fully adapted for first-person dashcam footage or accident scenarios. </p>
<img width="1024" height="1536" alt="Vehicular Incident Description Templates" src="https://github.com/user-attachments/assets/06d0b7a7-c070-430e-9b0e-8f2a4a76f1ba" />

<p>Another gap is the lack of large-scale, realistic datasets for video-based event understanding in traffic safety. While Nguyen et al. recently released OpenEvents V1, a massive image-text benchmark for event grounding[3], it is focused on news imagery and narrative text, not on dynamic road scenes. Similarly, Wang et al.’s ITFormer framework demonstrates how to bridge time-series signals with language for QA tasks[4], but an analogous bridge between continuous dashcam video streams and natural language has not been realized. In short, the community lacks an equivalent large-scale resource for video events in traffic. Skender et al. explicitly highlight this data scarcity: they resort to simulated DeepAccident videos because real-world accident footage is rare, noting the “scarcity of diverse, realistic, infrastructure-based accident data”[5]. In the dashcam context, no public dataset offers both annotated crash/near-miss video segments and rich textual descriptions, leaving models undertrained for real incidents. </p>

<img width="1024" height="1536" alt="Model_Architecture_Image " src="https://github.com/user-attachments/assets/7ea6615e-0d51-4fc5-aa4e-e06f23279c90" />


<p>A third gap concerns near-miss incidents and contextual understanding. Traffic safety research traditionally emphasizes actual crashes, often overlooking near-misses despite their predictive value. For example, Jaradat et al. stress that near-miss events “offer crucial predictive insights” but are “underrepresented in traffic safety research,” creating a “significant gap” in early hazard detection[6][7]. In addition, there is little work on generating explanatory narratives for detected events. Existing methods either output raw labels (e.g. “collision”) or simple captions; they do not provide coherent descriptions of why or how an incident occurred. In summary, current literature calls for systems that can ingest dashcam video, detect both crashes and near-misses, and automatically produce human-readable incident narratives, but this combination remains largely unexplored[6][1].
Contributions to Knowledge. </p>

<p>This work addresses the above gaps by introducing a novel, end-to-end multimodal pipeline for dashcam incident detection and explanation. The main contributions are as follows:
2COOOL Dataset and EDA. We compile the 2COOOL dataset, a curated collection of first-person dashcam videos annotated with incident labels (crash, near-miss, or normal driving) and accompanied by synthetic narrative descriptions. Unlike previous image-only event corpora[3], 2COOOL spans continuous driving footage and explicitly includes near-miss events. We conduct a comprehensive exploratory data analysis (Fig. 1) to characterize video features (optical flow, edge density, brightness, etc.) across incident types, revealing the spatiotemporal patterns that underpin accidents. This dataset and analysis fill the empirical gap noted by Jaradat et al.[6] and provide a new benchmark for multimodal traffic event understanding. </p>

<p>Multimodal Incident Pipeline (Algorithm 3/Table 1.0). We develop an integrated processing pipeline (detailed in Algorithm 3) that extracts spatiotemporal features from each video clip and classifies incidents using machine learning. VideoFeatureExtractor computes motion (via optical flow and frame differences), edge density, and brightness changes, capturing temporal context that single-image classifiers would miss. This design follows Jaradat et al.’s insight that architectures like CNN+LSTM and Vision Transformers can capture video dynamics for incident classification[8]. Our pipeline then applies ensemble models (e.g. Random Forest, Gradient Boosting) and deep networks to these features to detect events. Unlike prior work focused on binary crash detection, we perform three-way classification (crash vs. near-miss vs. normal) to provide finer-grained situational awareness, addressing the triplet of labels introduced in Jaradat et al.[9]. </p>

<p>Narrative Explanation via Text Generation. In addition to detection, we incorporate a narrative generation module (ImprovedTextGenerator) to explain incidents in natural language. This module uses a mix of rule-based templates and LLM prompts to produce pre-incident context (“The car is on a highway with light traffic”) and post-incident descriptions (“A truck suddenly brakes ahead of the ego vehicle”). By synthesizing language about road type, traffic conditions, and incident causes, we bridge the gap between raw video analysis and human-understandable explanations. This extends the descriptive capabilities of prior dashcam captioning efforts (e.g., Rafiq et al.’s Camscribe) with structured accident reasoning. In line with Jaradat et al.’s use of GPT-4 for event narratives[10], our pipeline demonstrates how combining vision features with language yields richer event interpretations.
Efficient System Design. We introduce a custom memory management strategy to enable near-real-time operation. The MemoryManager aggressively cleans up GPU and CPU resources between video segments, ensuring scalability for long dashcam streams. This practical innovation addresses the computational challenges noted in MLLM-based systems[1][2] and is critical for deployment in edge computing environments. </p>

</p>Empirical Validation. We evaluate the full 2COOOL pipeline on our dataset and show improved performance over baseline models. By leveraging temporal features and narrative context, our system achieves higher incident detection accuracy than single-frame classifiers. These results support the emerging consensus that multimodal integration enhances traffic safety analysis[2][6]. We also demonstrate that the generated descriptions align well with ground truth narratives, providing interpretable insights into each incident.
Collectively, these contributions advance understanding in several ways. For computer vision, we show that new spatiotemporal features (optical-flow, etc.) significantly aid incident classification. For transportation safety, we underscore the value of incorporating near-miss events and proactive explanations into automated monitoring. For multimodal AI, we provide a concrete example of fusing video and language modalities in a safety-critical domain, much as ITFormer bridges signals with text[4]. In doing so, our work lays the groundwork for future systems that can perceive and describe traffic events end-to-end.
References: The assertions above are grounded in current research. For instance, Nguyen et al. emphasize event-level grounding in multimodal datasets[3], and Wang et al. illustrate bridging temporal data with language[4], indicating the importance of our multimodal approach. Skender et al. highlight data scarcity and the use of simulation[5], justifying our dataset creation. Zhang et al. introduce an MLLM-based framework (SeeUnsafe) for accident analysis[1], which inspires our end-to-end philosophy. Jaradat et al. demonstrate the predictive power of near-misses and LLM narration[6][8], aligning with our focus on nuanced crash understanding and explanation. By addressing these documented gaps, our 2COOOL framework makes a novel contribution to the fields of computer vision and transportation safety. </p>

<h1>REFERENCES </h1>

<p>Original Dataset provided by:
@article{alshami20252coool, title = {2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving}, author = {AlShami, Ali K, Rabinowitz, Ryan, Shoman, Maged, Fang, Jianwu, Picek, Lukas, Lo, Shao-Yuan, Cruz, Steve, Lam, Khang Nhut, Kamod, Nachiket, Li, Lei-Lei, others}, journal = {arXiv preprint arXiv:2508.21080}, year = {2025} } ---------------------------------------------------------------------------------------------------------------- @article{alshami2024coool, title = {COOOL: Challenge of Out-of-Label -- A Novel Benchmark for Autonomous Driving}, author = {AlShami, Ali K, Kalita, Ananya, Rabinowitz, Ryan, Lam, Khang, Bezbarua, Rishabh, Boult, Terrance, and Kalita, Jugal}, journal = {arXiv preprint arXiv:2412.05462}, year = {2024} } . 2COOOL. https://kaggle.com/competitions/2coool, 2025. Kaggle.</p>

<img width="5370" height="3543" alt="eda_analysis" src="https://github.com/user-attachments/assets/7cd87a2b-7cf6-4b23-a4ed-8b8bb64e83d9" />


<p>
[1] [2501.10604] When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis
https://arxiv.org/abs/2501.10604
[2] [2504.16134] Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends
https://arxiv.org/abs/2504.16134
[3] [2506.18372] OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding
https://arxiv.org/abs/2506.18372
[4] [2506.20093] ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset
https://arxiv.org/abs/2506.20093
[5] [2509.19096v2] Investigating Traffic Accident Detection Using Multimodal Large Language Models
https://arxiv.org/abs/2509.19096v2?utm_source=agent-k&utm_medium=email&utm_campaign=llm-daily-september-25-2025
[6] [7] [8] [9] [10] (PDF) Leveraging Deep Learning and Multimodal Large Language Models for Near-Miss Detection Using Crowdsourced Videos
https://www.researchgate.net/publication/387718170_Leveraging_Deep_Learning_and_Multimodal_Large_Language_Models_for_Near-Miss_Detection_Using_Crowdsourced_Videos </p>


