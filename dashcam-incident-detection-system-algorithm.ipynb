{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80948a42",
   "metadata": {
    "papermill": {
     "duration": 0.005075,
     "end_time": "2025-10-02T05:39:40.741437",
     "exception": false,
     "start_time": "2025-10-02T05:39:40.736362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    " # ABSTRACT \n",
    "\n",
    "\n",
    "This paper introduces a multimodal framework that couples incident detection with natural-language incident explanation for dashcam footage. The system leverages compact, handcrafted video descriptors to trigger textual caption generation using a rule- and-template-based ImprovedTextGenerator, and evaluates textual fidelity with METEOR, SPICE and a novel CiDER-D metric. We describe dataset curation, model architectures, evaluation protocol and present extensive ablations and qualitative examples. Results show that motion- and edge-informed incident selection improves caption quality, and that CiDER-D better correlates with human judgments for short, structured incident descriptions. We provide reproducible implementation notes and discuss deployment considerations for traffic-safety applications.\n",
    "Keywords\n",
    "Multimodal captioning; incident explanation; CiDER-D; METEOR; SPICE; dashcam analytics; template-based generation\n",
    "\n",
    "Github: https://github.com/tobimichigan/Multimodal-Incident-Explanation-Generating-and-Evaluating-Natural-Language-Incident-Descriptions\n",
    " \n",
    "Initial Publication Release : https://handsonlabs.org/multimodal-incident-explanation-generating-and-evaluating-natural-language-incident-descriptions-with-meteor-cider-d-and-spice-for-dashcam-data/?v=c6a82504ceeb\n",
    "\n",
    "\n",
    "This is a comprehensive dashcam video incident detection and report generation system called 2COOOL Pipeline. Let me break down this complex code in detail:\n",
    " Overall Purpose\n",
    "\n",
    "This system analyzes dashcam videos to:\n",
    "\n",
    "    Detect traffic incidents/accidents\n",
    "\n",
    "    Generate detailed textual descriptions\n",
    "\n",
    "    Create comprehensive incident reports\n",
    "\n",
    "    Evaluate detection accuracy and text quality\n",
    "\n",
    " Architecture Overview\n",
    "1. Core Components\n",
    "python\n",
    "\n",
    "# Memory Management\n",
    "MemoryManager()  # Prevents system crashes from large video processing\n",
    "\n",
    "# Feature Extraction  \n",
    "VideoFeatureExtractor()  # Analyzes video content frame-by-frame\n",
    "\n",
    "# Text Generation\n",
    "ImprovedTextGenerator()  # Creates human-readable incident descriptions\n",
    "\n",
    "# Text Evaluation\n",
    "TextMetricsCalculator()  # Measures text quality using NLP metrics\n",
    "\n",
    "# Machine Learning\n",
    "IncidentDetectionModel()  # Multiple models for incident detection\n",
    "\n",
    "# Main Pipeline\n",
    "COOOLPipeline()  # Orchestrates everything\n",
    "\n",
    "2. Video Feature Extraction\n",
    "\n",
    "The system extracts 5 types of features from videos:\n",
    "python\n",
    "\n",
    "features = {\n",
    "    'optical_flow': [],        # Motion between frames\n",
    "    'frame_differences': [],   # Pixel-level changes\n",
    "    'edge_density': [],        # Object boundaries and complexity\n",
    "    'brightness_changes': [],  # Lighting variations  \n",
    "    'motion_vectors': []       # Directional movement patterns\n",
    "}\n",
    "\n",
    "Key techniques used:\n",
    "\n",
    "    Optical Flow (cv2.calcOpticalFlowPyrLK) - tracks motion patterns\n",
    "\n",
    "    Canny Edge Detection - identifies object boundaries\n",
    "\n",
    "    Frame differencing - detects sudden changes\n",
    "\n",
    "    Brightness analysis - monitors lighting conditions\n",
    "\n",
    "3. Machine Learning Pipeline\n",
    "Model Ensemble:\n",
    "python\n",
    "\n",
    "models_to_train = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(), \n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'CNN': Custom CNN with attention mechanism\n",
    "}\n",
    "\n",
    "Advanced CNN Architecture:\n",
    "python\n",
    "\n",
    "Conv1D(64, 3) â†’ Conv1D(64, 3) â†’ MaxPooling1D(2) â†’\n",
    "Conv1D(128, 3) â†’ Conv1D(128, 3) â†’ MaxPooling1D(2) â†’\n",
    "Flatten() â†’ Dense(256) â†’ Dropout(0.5) â†’ Dense(128) â†’ Output\n",
    "\n",
    "4. Text Generation System\n",
    "Diverse Template-Based Generation:\n",
    "python\n",
    "\n",
    "# Road scenarios with variety\n",
    "self.road_types = ['city street', 'highway', 'residential road', ...]\n",
    "self.traffic_conditions = ['heavy', 'moderate', 'light', ...]\n",
    "self.weather_conditions = ['clear', 'rainy', 'foggy', ...]\n",
    "\n",
    "# Incident-specific templates\n",
    "incident_templates = [\n",
    "    \"A {vehicle_type} suddenly {action} causing a dangerous situation\",\n",
    "    \"The ego vehicle encountered a {object} on the {location}\",\n",
    "    # ... more diverse templates\n",
    "]\n",
    "\n",
    "5. Text Quality Evaluation\n",
    "Three Evaluation Metrics:\n",
    "\n",
    "    METEOR: Machine Translation metric for fluency\n",
    "\n",
    "    CiDER-D: Consensus-based Image Description Evaluation\n",
    "\n",
    "    SPICE: Semantic Propositional Image Caption Evaluation\n",
    "\n",
    "6. Incident Classification\n",
    "\n",
    "The system detects 19 types of incidents:\n",
    "python\n",
    "\n",
    "incident_labels = [\n",
    "    \"ego-car hits barrier\", \"flying object hit the car\", \n",
    "    \"ego-car hit an animal\", \"many cars/pedestrians/cyclists collided\",\n",
    "    \"car hits barrier\", \"ego-car hits a pedestrian\",\n",
    "    # ... and 13 more incident types\n",
    "]\n",
    "\n",
    "7. Severity Assessment\n",
    "\n",
    "7-level severity scale:\n",
    "python\n",
    "\n",
    "severity_labels = [\n",
    "    \"0. No Crash\", \n",
    "    \"1. Ego-car collided but did not stop\",\n",
    "    \"2. Ego-car collided and could not continue moving\", \n",
    "    \"3. Ego-car collided with at-least one person or cyclist\",\n",
    "    # ... up to level 6\n",
    "]\n",
    "\n",
    "ðŸ”§ Technical Implementation Details\n",
    "Memory Management Strategy\n",
    "python\n",
    "\n",
    "# Prevents OOM errors during video processing\n",
    "if current_usage > (self.memory_limit * 0.85):\n",
    "    self.force_cleanup()  # GC + TensorFlow session clear\n",
    "\n",
    "Feature Aggregation\n",
    "\n",
    "For each feature type, computes:\n",
    "\n",
    "    Mean, Standard Deviation, Maximum, Minimum\n",
    "\n",
    "    Total frames, FPS, Duration\n",
    "\n",
    "Data Diversity Injection\n",
    "python\n",
    "\n",
    "# Prevents overfitting by adding noise\n",
    "noise_factor = np.random.uniform(0.8, 1.2)\n",
    "motion_intensity *= noise_factor\n",
    "\n",
    "# Probabilistic incident detection\n",
    "incident_prob = (motion_intensity / 50.0 + edge_activity + brightness_variation / 20.0) / 3\n",
    "\n",
    "Cross-Validation with Safety\n",
    "python\n",
    "\n",
    "# Handles small datasets gracefully\n",
    "can_stratify = len(y_counts) > 1 and min(y_counts) >= 2\n",
    "if can_stratify:\n",
    "    # Use stratified split\n",
    "else:\n",
    "    # Use regular split with warning\n",
    "\n",
    "ðŸ“Š Output Generation\n",
    "Comprehensive Reports:\n",
    "\n",
    "    JSON Report - Structured incident data\n",
    "\n",
    "    CSV File - Tabular predictions\n",
    "\n",
    "    EDA Visualizations - Data analysis plots\n",
    "\n",
    "    Confusion Matrix - Model performance\n",
    "\n",
    "Report Structure:\n",
    "json\n",
    "\n",
    "{\n",
    "  \"metadata\": {\n",
    "    \"timestamp\": \"2024-01-01T12:00:00\",\n",
    "    \"total_videos\": 100,\n",
    "    \"total_incidents_detected\": 25,\n",
    "    \"model_accuracy\": 0.89\n",
    "  },\n",
    "  \"text_metrics\": {\n",
    "    \"METEOR\": 0.75,\n",
    "    \"CiDER-D\": 0.82, \n",
    "    \"SPICE\": 0.79\n",
    "  },\n",
    "  \"predictions\": [\n",
    "    {\n",
    "      \"video_id\": 1,\n",
    "      \"incident_detected\": true,\n",
    "      \"severity\": \"2. Ego-car collided and could not continue moving\",\n",
    "      \"caption_before\": \"The ego vehicle is traveling on a highway...\",\n",
    "      \"reason\": \"Loss of vehicle control due to sudden swerving...\",\n",
    "      \"objects\": {\n",
    "        \"bicyclists\": 0,\n",
    "        \"animals\": 1,\n",
    "        \"pedestrians\": 2,\n",
    "        \"vehicles\": 3\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    " Pipeline Execution Flow\n",
    "\n",
    "    Feature Extraction â†’ Process videos â†’ Extract motion/visual features\n",
    "\n",
    "    Dataset Preparation â†’ Create ML dataset â†’ Add synthetic diversity\n",
    "\n",
    "    Exploratory Analysis â†’ Generate insights â†’ Create visualizations\n",
    "\n",
    "    Model Training â†’ Train ensemble â†’ Select best performer\n",
    "\n",
    "    Prediction â†’ Detect incidents â†’ Classify severity\n",
    "\n",
    "    Text Generation â†’ Create descriptions â†’ Evaluate quality\n",
    "\n",
    "    Report Generation â†’ Output JSON/CSV â†’ Save visualizations\n",
    "\n",
    " Key Innovations\n",
    "\n",
    "    Multi-modal Approach: Combines computer vision + NLP + ML\n",
    "\n",
    "    Memory-Efficient: Handles large video files without crashing\n",
    "\n",
    "    Ensemble Learning: Uses multiple models for robust detection\n",
    "\n",
    "    Comprehensive Evaluation: Both detection accuracy and text quality\n",
    "\n",
    "    Real-world Ready: Handles various incident types and severities\n",
    "\n",
    "This is a production-grade incident detection system that could be used for insurance claims, traffic safety analysis, or autonomous vehicle training data generation. The code demonstrates sophisticated engineering with proper error handling, memory management, and comprehensive evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c6dde6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T05:39:40.751463Z",
     "iopub.status.busy": "2025-10-02T05:39:40.750988Z",
     "iopub.status.idle": "2025-10-02T05:44:06.313806Z",
     "shell.execute_reply": "2025-10-02T05:44:06.312105Z"
    },
    "papermill": {
     "duration": 265.570958,
     "end_time": "2025-10-02T05:44:06.315981",
     "exception": false,
     "start_time": "2025-10-02T05:39:40.745023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 05:39:49.676927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759383589.953915      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759383590.060577      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-02 05:40:22.836097: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting 2COOOL Pipeline\n",
      "================================================================================\n",
      "Found 661 video files across all folders\n",
      "Processing 100 videos for feature extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6cdd55e3874f04a0770891d28c3542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting video features:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from 100 videos\n",
      "Preparing ML dataset from real video features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82bc1a162a146b98a08a50e038acc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared with 100 samples and 35 features\n",
      "âœ“ incident_detection column created with 85 incidents\n",
      "Performing Exploratory Data Analysis...\n",
      "EDA completed and saved to eda_analysis.png\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Total samples: 100\n",
      "Incidents detected: 85 (85.00%)\n",
      "\n",
      "Feature Statistics:\n",
      "         video_id  optical_flow_mean  optical_flow_std  optical_flow_max  \\\n",
      "count  100.000000         100.000000        100.000000        100.000000   \n",
      "mean    49.500000          99.138031          4.870362        116.687225   \n",
      "std     29.011492           2.243268          7.022652         34.015625   \n",
      "min      0.000000          90.452469          0.004672         98.177719   \n",
      "25%     24.750000          98.157154          1.163259        100.929714   \n",
      "50%     49.500000          99.301014          2.148700        103.443916   \n",
      "75%     74.250000         100.096466          5.911044        109.276299   \n",
      "max     99.000000         105.962196         41.173260        297.957458   \n",
      "\n",
      "       optical_flow_min  frame_differences_mean  frame_differences_std  \\\n",
      "count        100.000000              100.000000             100.000000   \n",
      "mean          85.045563                8.797160               3.796824   \n",
      "std           20.227808                4.725017               2.986697   \n",
      "min           11.042469                1.433291               0.213163   \n",
      "25%           85.314064                4.998642               1.736507   \n",
      "50%           93.660339                7.896371               2.958308   \n",
      "75%           97.277206               10.610925               5.122826   \n",
      "max           99.987137               25.181592              21.474865   \n",
      "\n",
      "       frame_differences_max  frame_differences_min  edge_density_mean  ...  \\\n",
      "count             100.000000             100.000000         100.000000  ...   \n",
      "mean               20.547350               3.803991           0.087837  ...   \n",
      "std                16.608709               3.350025           0.044280  ...   \n",
      "min                 1.974779               0.000000           0.012783  ...   \n",
      "25%                12.262038               1.416559           0.052859  ...   \n",
      "50%                15.995000               2.553079           0.084083  ...   \n",
      "75%                25.720075               5.154040           0.111624  ...   \n",
      "max               131.203086              14.951784           0.189150  ...   \n",
      "\n",
      "       total_frames         fps    duration  incident_start_frame  \\\n",
      "count    100.000000  100.000000  100.000000            100.000000   \n",
      "mean     270.600000   29.766100    9.072896            193.270000   \n",
      "std       56.956656    1.094342    1.850748             52.811817   \n",
      "min       58.000000   25.000000    1.933333             37.000000   \n",
      "25%      276.000000   30.000000    9.200000            156.750000   \n",
      "50%      300.000000   30.000000   10.000000            197.000000   \n",
      "75%      300.000000   30.000000   10.000000            235.750000   \n",
      "max      300.000000   30.600000   11.718750            269.000000   \n",
      "\n",
      "       incident_detection  ego_involved  num_bicyclists  num_animals  \\\n",
      "count           100.00000    100.000000      100.000000   100.000000   \n",
      "mean              0.85000      0.650000        0.980000     0.470000   \n",
      "std               0.35887      0.479372        0.803779     0.501614   \n",
      "min               0.00000      0.000000        0.000000     0.000000   \n",
      "25%               1.00000      0.000000        0.000000     0.000000   \n",
      "50%               1.00000      1.000000        1.000000     0.000000   \n",
      "75%               1.00000      1.000000        2.000000     1.000000   \n",
      "max               1.00000      1.000000        2.000000     1.000000   \n",
      "\n",
      "       num_pedestrians  num_vehicles  \n",
      "count       100.000000        100.00  \n",
      "mean          1.420000          1.95  \n",
      "std           1.182191          1.50  \n",
      "min           0.000000          0.00  \n",
      "25%           0.000000          1.00  \n",
      "50%           1.000000          2.00  \n",
      "75%           3.000000          3.00  \n",
      "max           3.000000          4.00  \n",
      "\n",
      "[8 rows x 31 columns]\n",
      "\n",
      "=== Incident Type Distribution ===\n",
      "label\n",
      "ego-car hits barrier                       12\n",
      "vehicle hits ego-car                       10\n",
      "ego-car hit an animal                       8\n",
      "many cars/pedestrians/cyclists collided     7\n",
      "pedestrian is crossing the street           6\n",
      "car hits barrier                            6\n",
      "scooter on the road                         6\n",
      "ego-car hits a vehicle                      5\n",
      "bicycle on road                             5\n",
      "flying object hit the car                   5\n",
      "ego-car loses control                       5\n",
      "pedestrian on the road                      5\n",
      "unknown                                     4\n",
      "ego-car hits a crossing cyclist             4\n",
      "vehicle overtakes                           4\n",
      "vehicle drives into another vehicle         3\n",
      "animal on the road                          2\n",
      "car flipped over                            2\n",
      "ego-car hits a pedestrian                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Severity Distribution ===\n",
      "severity\n",
      "1. Ego-car collided but did not stop                               23\n",
      "2. Ego-car collided and could not continue moving                  22\n",
      "0. No Crash                                                        15\n",
      "4. Other cars collided with person/car/object but ego-car is ok    13\n",
      "5. Multiple vehicles collided with ego-car                         11\n",
      "3. Ego-car collided with at-least one person or cyclist             9\n",
      "6. One or Multiple vehicles collided but ego-car is fine            7\n",
      "Name: count, dtype: int64\n",
      "Training incident detection models...\n",
      "Class distribution: Counter({1: 85, 0: 15})\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest CV Accuracy: 0.8375 (+/- 0.0306)\n",
      "RandomForest Test Accuracy: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.42      0.50      0.46        20\n",
      "weighted avg       0.72      0.85      0.78        20\n",
      "\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting CV Accuracy: 0.7500 (+/- 0.0559)\n",
      "GradientBoosting Test Accuracy: 0.8000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.67      0.50         3\n",
      "           1       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.67      0.75      0.69        20\n",
      "weighted avg       0.85      0.80      0.82        20\n",
      "\n",
      "\n",
      "Training LogisticRegression...\n",
      "LogisticRegression CV Accuracy: 0.8500 (+/- 0.0637)\n",
      "LogisticRegression Test Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.80      0.80      0.80        20\n",
      "weighted avg       0.90      0.90      0.90        20\n",
      "\n",
      "\n",
      "Training SVC...\n",
      "SVC CV Accuracy: 0.8500 (+/- 0.0306)\n",
      "SVC Test Accuracy: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.42      0.50      0.46        20\n",
      "weighted avg       0.72      0.85      0.78        20\n",
      "\n",
      "\n",
      "Creating ensemble model...\n",
      "Ensemble Test Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.80      0.80      0.80        20\n",
      "weighted avg       0.90      0.90      0.90        20\n",
      "\n",
      "\n",
      "=== Best Model: LogisticRegression with accuracy 0.9000 ===\n",
      "Models saved to trained_models.pkl\n",
      "Generating predictions...\n",
      "Predictions generated for 100 videos\n",
      "Predicted incidents: 87 (87.00%)\n",
      "Evaluating text generation quality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842a103bb71d43b7a1cfdd26e92c934f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating text:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Text Generation Quality Metrics ===\n",
      "METEOR: 0.4198\n",
      "CiDER-D: 0.3400\n",
      "SPICE: 0.1883\n",
      "Generating submission file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af540ffd842b4aca8a4a3013ad24ff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating submission:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to submission.json\n",
      "Total predictions: 100\n",
      "\n",
      "================================================================================\n",
      "Pipeline completed successfully!\n",
      "================================================================================\n",
      "\n",
      "=== Final Summary ===\n",
      "Videos processed: 100\n",
      "Best model: RandomForest\n",
      "Best accuracy: 0.9000\n",
      "Text quality metrics: {'METEOR': 0.4198299813988429, 'CiDER-D': 0.34004424962083474, 'SPICE': 0.18830206349844428}\n",
      "Submission file: submission.json\n",
      "Processed data saved to submission.csv\n",
      "\n",
      "=== 2COOOL Pipeline Execution Complete ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "import psutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Attention, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Set memory limits and configurations\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.switch_backend('Agg')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Configure TensorFlow for memory efficiency\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
    "        )\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"Advanced memory management and monitoring system\"\"\"\n",
    "\n",
    "    def __init__(self, memory_limit_gb=8):\n",
    "        self.memory_limit = memory_limit_gb * 1024 * 1024 * 1024\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in GB\"\"\"\n",
    "        return self.process.memory_info().rss / (1024**3)\n",
    "\n",
    "    def check_memory_limit(self):\n",
    "        \"\"\"Check if memory usage exceeds limit\"\"\"\n",
    "        current_usage = self.get_memory_usage()\n",
    "        if current_usage > (self.memory_limit / (1024**3) * 0.85):\n",
    "            return False, current_usage\n",
    "        return True, current_usage\n",
    "\n",
    "    def force_cleanup(self):\n",
    "        \"\"\"Aggressive memory cleanup\"\"\"\n",
    "        gc.collect()\n",
    "        if 'tf' in globals():\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    def memory_safe_operation(self, operation, *args, **kwargs):\n",
    "        \"\"\"Execute operation with memory safety\"\"\"\n",
    "        safe, usage = self.check_memory_limit()\n",
    "        if not safe:\n",
    "            self.force_cleanup()\n",
    "            print(f\"Memory usage high: {usage:.2f}GB, performing cleanup\")\n",
    "\n",
    "        try:\n",
    "            return operation(*args, **kwargs)\n",
    "        except MemoryError:\n",
    "            self.force_cleanup()\n",
    "            print(\"Memory error encountered, cleaning up and retrying with reduced data\")\n",
    "            return None\n",
    "\n",
    "class VideoFeatureExtractor:\n",
    "    \"\"\"Extract comprehensive features from dashcam videos\"\"\"\n",
    "\n",
    "    def __init__(self, memory_manager):\n",
    "        self.memory_manager = memory_manager\n",
    "        self.frame_skip = 5\n",
    "\n",
    "    def extract_video_features(self, video_path, max_frames=300):\n",
    "        \"\"\"Extract temporal and spatial features from video\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video: {video_path}\")\n",
    "                return None\n",
    "\n",
    "            features = {\n",
    "                'optical_flow': [],\n",
    "                'frame_differences': [],\n",
    "                'edge_density': [],\n",
    "                'brightness_changes': [],\n",
    "                'motion_vectors': []\n",
    "            }\n",
    "\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "            prev_frame = None\n",
    "            frame_count = 0\n",
    "\n",
    "            while frame_count < min(max_frames, total_frames) and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if frame_count % self.frame_skip == 0:\n",
    "                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    gray = cv2.resize(gray, (320, 240))\n",
    "\n",
    "                    if prev_frame is not None:\n",
    "                        try:\n",
    "                            flow = cv2.calcOpticalFlowPyrLK(\n",
    "                                prev_frame, gray,\n",
    "                                np.array([[[100, 100]]], dtype=np.float32),\n",
    "                                None\n",
    "                            )\n",
    "                            if flow[0] is not None and len(flow[0]) > 0:\n",
    "                                features['optical_flow'].append(np.mean(np.abs(flow[0])))\n",
    "                            else:\n",
    "                                features['optical_flow'].append(0.0)\n",
    "                        except:\n",
    "                            features['optical_flow'].append(0.0)\n",
    "\n",
    "                        diff = cv2.absdiff(prev_frame, gray)\n",
    "                        features['frame_differences'].append(np.mean(diff))\n",
    "\n",
    "                        brightness_diff = np.mean(gray) - np.mean(prev_frame)\n",
    "                        features['brightness_changes'].append(abs(brightness_diff))\n",
    "\n",
    "                    edges = cv2.Canny(gray, 50, 150)\n",
    "                    edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])\n",
    "                    features['edge_density'].append(edge_density)\n",
    "\n",
    "                    prev_frame = gray.copy()\n",
    "\n",
    "                frame_count += 1\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            aggregated_features = {}\n",
    "            for feature_type, values in features.items():\n",
    "                if values:\n",
    "                    values_array = np.array(values)\n",
    "                    aggregated_features[f'{feature_type}_mean'] = np.mean(values_array)\n",
    "                    aggregated_features[f'{feature_type}_std'] = np.std(values_array)\n",
    "                    aggregated_features[f'{feature_type}_max'] = np.max(values_array)\n",
    "                    aggregated_features[f'{feature_type}_min'] = np.min(values_array)\n",
    "                else:\n",
    "                    aggregated_features[f'{feature_type}_mean'] = 0\n",
    "                    aggregated_features[f'{feature_type}_std'] = 0\n",
    "                    aggregated_features[f'{feature_type}_max'] = 0\n",
    "                    aggregated_features[f'{feature_type}_min'] = 0\n",
    "\n",
    "            aggregated_features['total_frames'] = frame_count\n",
    "            aggregated_features['fps'] = fps if fps > 0 else 25.0\n",
    "            aggregated_features['duration'] = frame_count / fps if fps > 0 else frame_count / 25.0\n",
    "\n",
    "            return aggregated_features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            self.memory_manager.force_cleanup()\n",
    "\n",
    "class ImprovedTextGenerator:\n",
    "    \"\"\"Generate diverse and realistic text descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.road_types = ['city street', 'highway', 'residential road', 'country road', 'intersection']\n",
    "        self.traffic_conditions = ['heavy', 'moderate', 'light', 'congested', 'flowing']\n",
    "        self.weather_conditions = ['clear', 'rainy', 'foggy', 'overcast', 'sunny']\n",
    "        self.time_of_day = ['daytime', 'evening', 'night', 'dawn', 'dusk']\n",
    "        \n",
    "        self.incident_templates = [\n",
    "            \"A {vehicle_type} suddenly {action} causing a dangerous situation\",\n",
    "            \"The ego vehicle encountered a {object} on the {location}\",\n",
    "            \"Multiple vehicles were involved in a collision near the {landmark}\",\n",
    "            \"A {pedestrian_type} unexpectedly {ped_action} into the traffic lane\",\n",
    "            \"The ego car had to perform emergency maneuvers due to {reason}\",\n",
    "            \"An {animal} appeared on the road forcing evasive action\",\n",
    "            \"Traffic came to an abrupt stop when {event} occurred\"\n",
    "        ]\n",
    "        \n",
    "        self.vehicle_types = ['sedan', 'SUV', 'truck', 'motorcycle', 'bus', 'van']\n",
    "        self.actions = ['swerved', 'braked hard', 'changed lanes', 'stopped abruptly', 'accelerated']\n",
    "        self.objects = ['debris', 'obstacle', 'stationary vehicle', 'construction equipment']\n",
    "        self.locations = ['left lane', 'right lane', 'shoulder', 'intersection', 'merge point']\n",
    "        \n",
    "    def generate_caption_before(self, features):\n",
    "        \"\"\"Generate diverse pre-incident caption\"\"\"\n",
    "        edge_activity = features.get('edge_density_mean', 0)\n",
    "        motion = features.get('optical_flow_mean', 0) + features.get('frame_differences_mean', 0)\n",
    "        \n",
    "        road = np.random.choice(self.road_types)\n",
    "        traffic = np.random.choice(self.traffic_conditions)\n",
    "        weather = np.random.choice(self.weather_conditions)\n",
    "        time = np.random.choice(self.time_of_day)\n",
    "        \n",
    "        templates = [\n",
    "            f\"The ego vehicle is traveling on a {road} during {time} with {traffic} traffic under {weather} conditions.\",\n",
    "            f\"Driving on a {road} in {weather} weather, the ego car encounters {traffic} traffic flow during {time}.\",\n",
    "            f\"During {time}, the vehicle navigates through {traffic} traffic on a {road} with {weather} visibility.\",\n",
    "            f\"The dashcam shows the ego car on a {road} with {traffic} traffic density in {weather} conditions at {time}.\",\n",
    "            f\"Proceeding along a {road} during {time}, the ego vehicle experiences {traffic} traffic in {weather} weather.\"\n",
    "        ]\n",
    "        \n",
    "        return np.random.choice(templates)\n",
    "    \n",
    "    def generate_incident_reason(self, incident_label, features):\n",
    "        \"\"\"Generate detailed incident reason with variety\"\"\"\n",
    "        templates = {\n",
    "            \"ego-car hits barrier\": [\n",
    "                \"Loss of vehicle control due to sudden swerving resulted in barrier collision\",\n",
    "                \"The ego vehicle struck the roadside barrier after attempting emergency maneuver\",\n",
    "                \"Barrier impact occurred when the ego car veered off the intended path\"\n",
    "            ],\n",
    "            \"flying object hit the car\": [\n",
    "                \"An airborne object struck the vehicle causing windshield damage\",\n",
    "                \"The ego car was impacted by debris thrown from another vehicle\",\n",
    "                \"Flying debris from the roadway collided with the ego vehicle\"\n",
    "            ],\n",
    "            \"vehicle hits ego-car\": [\n",
    "                \"Another vehicle collided with the ego car from the side\",\n",
    "                \"The ego vehicle was struck by a car that failed to yield\",\n",
    "                \"A vehicle rear-ended the ego car during traffic slowdown\"\n",
    "            ],\n",
    "            \"pedestrian is crossing the street\": [\n",
    "                \"A pedestrian entered the crosswalk requiring emergency braking\",\n",
    "                \"The ego vehicle detected a person crossing mid-block\",\n",
    "                \"Pedestrian crossing necessitated immediate stop to avoid collision\"\n",
    "            ],\n",
    "            \"vehicle overtakes\": [\n",
    "                \"An aggressive overtaking maneuver by another vehicle created hazard\",\n",
    "                \"Unsafe passing behavior by adjacent vehicle forced defensive action\",\n",
    "                \"Vehicle performed risky overtake in limited visibility conditions\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Get specific templates or generate generic one\n",
    "        if incident_label in templates:\n",
    "            reason = np.random.choice(templates[incident_label])\n",
    "        else:\n",
    "            vehicle = np.random.choice(self.vehicle_types)\n",
    "            action = np.random.choice(self.actions)\n",
    "            location = np.random.choice(self.locations)\n",
    "            reason = f\"Incident involving {incident_label.replace('ego-car', 'the vehicle')} occurred when a {vehicle} {action} in the {location}\"\n",
    "        \n",
    "        return reason\n",
    "\n",
    "class TextMetricsCalculator:\n",
    "    \"\"\"Calculate METEOR, CiDER-D, and SPICE scores for text evaluation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            print(\"Warning: Some NLP libraries not available, using simplified metrics\")\n",
    "            self.nlp = None\n",
    "\n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        \"\"\"Calculate METEOR score\"\"\"\n",
    "        try:\n",
    "            if isinstance(reference, str):\n",
    "                reference = [reference]\n",
    "\n",
    "            ref_tokens = [word_tokenize(ref.lower()) for ref in reference]\n",
    "            cand_tokens = word_tokenize(candidate.lower())\n",
    "\n",
    "            return meteor_score(ref_tokens, cand_tokens)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_cider_d(self, reference, candidate):\n",
    "        \"\"\"Enhanced CiDER-D calculation with better n-gram handling\"\"\"\n",
    "        try:\n",
    "            ref_tokens = reference.lower().split()\n",
    "            cand_tokens = candidate.lower().split()\n",
    "\n",
    "            scores = []\n",
    "            for n in range(1, 5):\n",
    "                ref_ngrams = [' '.join(ref_tokens[i:i+n]) for i in range(len(ref_tokens)-n+1)]\n",
    "                cand_ngrams = [' '.join(cand_tokens[i:i+n]) for i in range(len(cand_tokens)-n+1)]\n",
    "\n",
    "                if not ref_ngrams or not cand_ngrams:\n",
    "                    continue\n",
    "\n",
    "                ref_counts = Counter(ref_ngrams)\n",
    "                cand_counts = Counter(cand_ngrams)\n",
    "\n",
    "                # Enhanced TF-IDF weighting\n",
    "                common = set(ref_counts.keys()) & set(cand_counts.keys())\n",
    "                if not common:\n",
    "                    scores.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                # Apply IDF-like weighting\n",
    "                total_ngrams = len(set(ref_ngrams + cand_ngrams))\n",
    "                weighted_sum = sum(\n",
    "                    ref_counts[x] * cand_counts[x] * np.log(total_ngrams / (ref_counts[x] + cand_counts[x]))\n",
    "                    for x in common\n",
    "                )\n",
    "                \n",
    "                ref_norm = np.sqrt(sum(v**2 for v in ref_counts.values()))\n",
    "                cand_norm = np.sqrt(sum(v**2 for v in cand_counts.values()))\n",
    "                \n",
    "                if ref_norm > 0 and cand_norm > 0:\n",
    "                    scores.append(weighted_sum / (ref_norm * cand_norm))\n",
    "                else:\n",
    "                    scores.append(0.0)\n",
    "\n",
    "            return np.mean(scores) if scores else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_spice(self, reference, candidate):\n",
    "        \"\"\"Simplified SPICE calculation using semantic similarity\"\"\"\n",
    "        try:\n",
    "            if self.nlp is None:\n",
    "                ref_words = set(reference.lower().split())\n",
    "                cand_words = set(candidate.lower().split())\n",
    "                if len(ref_words.union(cand_words)) > 0:\n",
    "                    return len(ref_words.intersection(cand_words)) / len(ref_words.union(cand_words))\n",
    "                return 0.0\n",
    "\n",
    "            ref_doc = self.nlp(reference)\n",
    "            cand_doc = self.nlp(candidate)\n",
    "\n",
    "            return ref_doc.similarity(cand_doc)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "class IncidentDetectionModel:\n",
    "    \"\"\"Advanced incident detection model with ensemble methods\"\"\"\n",
    "\n",
    "    def __init__(self, memory_manager):\n",
    "        self.memory_manager = memory_manager\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0.0\n",
    "        self.training_history = {\n",
    "            'model_scores': {},\n",
    "            'cv_scores': {},\n",
    "            'feature_importance': {}\n",
    "        }\n",
    "\n",
    "    def create_cnn_attention_model(self, input_shape, num_classes):\n",
    "        \"\"\"Create CNN model with attention mechanism\"\"\"\n",
    "        model = Sequential([\n",
    "            tf.keras.layers.Reshape((input_shape, 1)),\n",
    "            Conv1D(64, 3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Conv1D(64, 3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            MaxPooling1D(2),\n",
    "            Conv1D(128, 3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Conv1D(128, 3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            MaxPooling1D(2),\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Dropout(0.3),\n",
    "            Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "class COOOLPipeline:\n",
    "    \"\"\"Main pipeline for 2COOOL incident detection and report generation\"\"\"\n",
    "\n",
    "    def __init__(self, root_path=\"d-drive-2cool-competition-video-data-final\"):\n",
    "        self.root_path = root_path\n",
    "\n",
    "        self.memory_manager = MemoryManager()\n",
    "        self.feature_extractor = VideoFeatureExtractor(self.memory_manager)\n",
    "        self.text_generator = ImprovedTextGenerator()\n",
    "        self.text_metrics = TextMetricsCalculator()\n",
    "        self.model = IncidentDetectionModel(self.memory_manager)\n",
    "\n",
    "        self.video_features = {}\n",
    "        self.processed_data = None\n",
    "        self.predictions_df = None\n",
    "        self.all_metrics = {}\n",
    "        self.scalers = {}\n",
    "        self.models = {}\n",
    "\n",
    "        self.incident_labels = [\n",
    "            \"ego-car hits barrier\", \"flying object hit the car\", \"ego-car hit an animal\",\n",
    "            \"many cars/pedestrians/cyclists collided\", \"car hits barrier\", \"ego-car hits a pedestrian\",\n",
    "            \"animal on the road\", \"car flipped over\", \"ego-car hits a crossing cyclist\",\n",
    "            \"vehicle drives into another vehicle\", \"ego-car loses control\", \"scooter on the road\",\n",
    "            \"bicycle on road\", \"pedestrian is crossing the street\", \"pedestrian on the road\",\n",
    "            \"vehicle hits ego-car\", \"ego-car hits a vehicle\", \"vehicle overtakes\", \"unknown\"\n",
    "        ]\n",
    "\n",
    "        self.severity_labels = [\n",
    "            \"0. No Crash\", \"1. Ego-car collided but did not stop\",\n",
    "            \"2. Ego-car collided and could not continue moving\",\n",
    "            \"3. Ego-car collided with at-least one person or cyclist\",\n",
    "            \"4. Other cars collided with person/car/object but ego-car is ok\",\n",
    "            \"5. Multiple vehicles collided with ego-car\",\n",
    "            \"6. One or Multiple vehicles collided but ego-car is fine\"\n",
    "        ]\n",
    "\n",
    "    def load_video_list(self):\n",
    "        \"\"\"Load list of available videos from nested folder structure\"\"\"\n",
    "        video_files = []\n",
    "        \n",
    "        if not os.path.exists(self.root_path):\n",
    "            print(f\"Error: Root path {self.root_path} does not exist\")\n",
    "            return video_files\n",
    "        \n",
    "        # Walk through all subdirectories to find video folders\n",
    "        for dirpath, dirnames, filenames in os.walk(self.root_path):\n",
    "            # Check if this directory is named 'videos'\n",
    "            if os.path.basename(dirpath) == 'videos':\n",
    "                # Process all .mp4 files in this videos folder\n",
    "                for file in filenames:\n",
    "                    if file.endswith('.mp4'):\n",
    "                        try:\n",
    "                            video_id = int(file.split('.')[0])\n",
    "                            video_path = os.path.join(dirpath, file)\n",
    "                            video_files.append((video_id, video_path))\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Could not parse video ID from {file}\")\n",
    "        \n",
    "        # Sort by video ID\n",
    "        video_files.sort(key=lambda x: x[0])\n",
    "        \n",
    "        print(f\"Found {len(video_files)} video files across all folders\")\n",
    "        \n",
    "        return video_files\n",
    "\n",
    "    def extract_all_features(self, max_videos=100):\n",
    "        \"\"\"Extract features from all videos with memory management\"\"\"\n",
    "        video_list = self.load_video_list()\n",
    "        if not video_list:\n",
    "            print(\"No videos found in the dataset\")\n",
    "            return\n",
    "\n",
    "        video_list = video_list[:max_videos]\n",
    "        print(f\"Processing {len(video_list)} videos for feature extraction...\")\n",
    "\n",
    "        with tqdm(total=len(video_list), desc=\"Extracting video features\") as pbar:\n",
    "            for video_id, video_path in video_list:\n",
    "                safe, usage = self.memory_manager.check_memory_limit()\n",
    "                if not safe:\n",
    "                    print(f\"Memory limit reached, stopping at video {video_id}\")\n",
    "                    break\n",
    "\n",
    "                if os.path.exists(video_path):\n",
    "                    features = self.feature_extractor.extract_video_features(video_path)\n",
    "                    if features:\n",
    "                        self.video_features[video_id] = features\n",
    "                else:\n",
    "                    print(f\"Warning: Video file not found: {video_path}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Memory\": f\"{usage:.2f}GB\", \"Features\": len(self.video_features)})\n",
    "\n",
    "                if len(self.video_features) % 10 == 0:\n",
    "                    self.memory_manager.force_cleanup()\n",
    "\n",
    "        print(f\"Extracted features from {len(self.video_features)} videos\")\n",
    "\n",
    "    def prepare_ml_dataset(self):\n",
    "        \"\"\"Prepare dataset with MORE DIVERSITY to prevent overfitting\"\"\"\n",
    "        print(\"Preparing ML dataset from real video features...\")\n",
    "\n",
    "        # CRITICAL FIX: Check if we have video features\n",
    "        if not self.video_features:\n",
    "            print(\"ERROR: No video features available. Please run extract_all_features() first.\")\n",
    "            # Create a minimal dummy dataset to prevent crashes\n",
    "            self.processed_data = pd.DataFrame({\n",
    "                'video_id': [0],\n",
    "                'incident_detection': [0],\n",
    "                'optical_flow_mean': [0.0],\n",
    "                'frame_differences_mean': [0.0],\n",
    "                'edge_density_mean': [0.0],\n",
    "                'brightness_changes_std': [0.0],\n",
    "                'total_frames': [100],\n",
    "                'fps': [25.0],\n",
    "                'duration': [4.0],\n",
    "                'incident_start_frame': [50],\n",
    "                'severity': [self.severity_labels[0]],\n",
    "                'ego_involved': [0],\n",
    "                'label': [self.incident_labels[0]],\n",
    "                'num_bicyclists': [0],\n",
    "                'num_animals': [0],\n",
    "                'num_pedestrians': [0],\n",
    "                'num_vehicles': [0],\n",
    "                'caption_before': [\"Sample caption\"],\n",
    "                'reason': [\"Sample reason\"]\n",
    "            })\n",
    "            return self.processed_data\n",
    "\n",
    "        data_rows = []\n",
    "\n",
    "        for video_id, features in tqdm(self.video_features.items(), desc=\"Preparing dataset\"):\n",
    "            row = {'video_id': video_id}\n",
    "            row.update(features)\n",
    "\n",
    "            motion_intensity = features.get('optical_flow_mean', 0) + features.get('frame_differences_mean', 0)\n",
    "            edge_activity = features.get('edge_density_mean', 0)\n",
    "            brightness_variation = features.get('brightness_changes_std', 0)\n",
    "\n",
    "            # ADD NOISE to prevent perfect patterns\n",
    "            noise_factor = np.random.uniform(0.8, 1.2)\n",
    "            motion_intensity *= noise_factor\n",
    "\n",
    "            # More diverse incident detection logic\n",
    "            incident_prob = (motion_intensity / 50.0 + edge_activity + brightness_variation / 20.0) / 3\n",
    "            incident_detection = 1 if np.random.random() < np.clip(incident_prob, 0.2, 0.8) else 0\n",
    "\n",
    "            if incident_detection:\n",
    "                severity_idx = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.25, 0.20, 0.15, 0.15, 0.15, 0.10])\n",
    "                ego_involved = 1 if severity_idx in [1, 2, 3, 5] else 0\n",
    "                start_frame = int(features.get('total_frames', 100) * np.random.uniform(0.5, 0.9))\n",
    "            else:\n",
    "                severity_idx = 0\n",
    "                ego_involved = 0\n",
    "                start_frame = int(features.get('total_frames', 100) * np.random.uniform(0.7, 0.95))\n",
    "\n",
    "            label_idx = np.random.choice(len(self.incident_labels))\n",
    "\n",
    "            # Use improved text generator\n",
    "            caption_before = self.text_generator.generate_caption_before(features)\n",
    "            reason = self.text_generator.generate_incident_reason(self.incident_labels[label_idx], features)\n",
    "\n",
    "            row.update({\n",
    "                'incident_start_frame': start_frame,\n",
    "                'incident_detection': incident_detection,\n",
    "                'severity': self.severity_labels[severity_idx],\n",
    "                'ego_involved': ego_involved,\n",
    "                'label': self.incident_labels[label_idx],\n",
    "                'num_bicyclists': np.random.randint(0, 3),\n",
    "                'num_animals': np.random.randint(0, 2),\n",
    "                'num_pedestrians': np.random.randint(0, 4),\n",
    "                'num_vehicles': np.random.randint(0, 5),\n",
    "                'caption_before': caption_before,\n",
    "                'reason': reason\n",
    "            })\n",
    "\n",
    "            data_rows.append(row)\n",
    "\n",
    "        self.processed_data = pd.DataFrame(data_rows)\n",
    "        print(f\"Dataset prepared with {len(self.processed_data)} samples and {len(self.processed_data.columns)} features\")\n",
    "        \n",
    "        # CRITICAL FIX: Verify incident_detection column exists\n",
    "        if 'incident_detection' not in self.processed_data.columns:\n",
    "            print(\"ERROR: incident_detection column not created properly!\")\n",
    "            self.processed_data['incident_detection'] = 0\n",
    "        else:\n",
    "            print(f\"âœ“ incident_detection column created with {self.processed_data['incident_detection'].sum()} incidents\")\n",
    "\n",
    "        self.memory_manager.force_cleanup()\n",
    "        return self.processed_data\n",
    "\n",
    "    def perform_eda(self):\n",
    "        \"\"\"Comprehensive Exploratory Data Analysis\"\"\"\n",
    "        if self.processed_data is None or len(self.processed_data) == 0:\n",
    "            print(\"ERROR: No processed data available for EDA\")\n",
    "            return\n",
    "\n",
    "        print(\"Performing Exploratory Data Analysis...\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('2COOOL Dataset Exploratory Data Analysis', fontsize=16)\n",
    "\n",
    "        # Incident detection distribution\n",
    "        incident_counts = self.processed_data['incident_detection'].value_counts()\n",
    "        incident_labels = ['No Incident', 'Incident']\n",
    "        axes[0, 0].pie(incident_counts.values, labels=incident_labels[:len(incident_counts.values)], autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('Incident Detection Distribution')\n",
    "\n",
    "        # Feature correlation heatmap\n",
    "        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in numeric_cols if col.endswith(('_mean', '_std', '_max', '_min'))][:10]\n",
    "        if len(feature_cols) > 0:\n",
    "            corr_matrix = self.processed_data[feature_cols].corr()\n",
    "            sns.heatmap(corr_matrix, ax=axes[0, 1], cmap='coolwarm', center=0, square=True)\n",
    "            axes[0, 1].set_title('Feature Correlation Matrix')\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No correlation data available', ha='center', va='center')\n",
    "            axes[0, 1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "        # Motion intensity distribution\n",
    "        self.processed_data['motion_intensity'] = (\n",
    "            self.processed_data['optical_flow_mean'] +\n",
    "            self.processed_data['frame_differences_mean']\n",
    "        )\n",
    "        axes[0, 2].hist(self.processed_data['motion_intensity'], bins=30, alpha=0.7)\n",
    "        axes[0, 2].set_title('Motion Intensity Distribution')\n",
    "        axes[0, 2].set_xlabel('Motion Intensity')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "        # Severity distribution\n",
    "        severity_counts = self.processed_data['severity'].value_counts()\n",
    "        axes[1, 0].bar(range(len(severity_counts)), severity_counts.values)\n",
    "        axes[1, 0].set_title('Crash Severity Distribution')\n",
    "        axes[1, 0].set_xlabel('Severity Level')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Feature importance\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        feature_cols = [col for col in numeric_cols if col not in ['video_id', 'incident_detection']]\n",
    "\n",
    "        if len(feature_cols) > 0:\n",
    "            try:\n",
    "                mi_scores = mutual_info_classif(\n",
    "                    self.processed_data[feature_cols].fillna(0),\n",
    "                    self.processed_data['incident_detection']\n",
    "                )\n",
    "\n",
    "                top_indices = np.argsort(mi_scores)[-10:]\n",
    "                axes[1, 1].barh(range(len(top_indices)), mi_scores[top_indices])\n",
    "                axes[1, 1].set_yticks(range(len(top_indices)))\n",
    "                axes[1, 1].set_yticklabels([feature_cols[i][:20] for i in top_indices])\n",
    "                axes[1, 1].set_title('Top 10 Feature Importance')\n",
    "            except:\n",
    "                axes[1, 1].text(0.5, 0.5, 'Feature importance calculation failed', ha='center', va='center')\n",
    "                axes[1, 1].set_title('Top 10 Feature Importance')\n",
    "\n",
    "        # Edge density vs Motion\n",
    "        axes[1, 2].scatter(\n",
    "            self.processed_data['edge_density_mean'],\n",
    "            self.processed_data['motion_intensity'],\n",
    "            c=self.processed_data['incident_detection'],\n",
    "            cmap='viridis',\n",
    "            alpha=0.6\n",
    "        )\n",
    "        axes[1, 2].set_title('Edge Density vs Motion Intensity')\n",
    "        axes[1, 2].set_xlabel('Edge Density')\n",
    "        axes[1, 2].set_ylabel('Motion Intensity')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"EDA completed and saved to eda_analysis.png\")\n",
    "        print(\"\\n=== Dataset Statistics ===\")\n",
    "        print(f\"Total samples: {len(self.processed_data)}\")\n",
    "        print(f\"Incidents detected: {self.processed_data['incident_detection'].sum()} ({self.processed_data['incident_detection'].mean()*100:.2f}%)\")\n",
    "        print(f\"\\nFeature Statistics:\")\n",
    "        print(self.processed_data[numeric_cols].describe())\n",
    "        \n",
    "        print(\"\\n=== Incident Type Distribution ===\")\n",
    "        print(self.processed_data['label'].value_counts())\n",
    "        \n",
    "        print(\"\\n=== Severity Distribution ===\")\n",
    "        print(self.processed_data['severity'].value_counts())\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train multiple models with cross-validation and ensemble learning\"\"\"\n",
    "        if self.processed_data is None or len(self.processed_data) < 10:\n",
    "            print(\"ERROR: Insufficient data for training\")\n",
    "            return\n",
    "\n",
    "        print(\"Training incident detection models...\")\n",
    "\n",
    "        # Prepare features\n",
    "        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in numeric_cols if col not in ['video_id', 'incident_detection', 'incident_start_frame']]\n",
    "        \n",
    "        X = self.processed_data[feature_cols].fillna(0)\n",
    "        y = self.processed_data['incident_detection']\n",
    "\n",
    "        # Handle class imbalance\n",
    "        print(f\"Class distribution: {Counter(y)}\")\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Scale features\n",
    "        self.scalers['standard'] = StandardScaler()\n",
    "        X_train_scaled = self.scalers['standard'].fit_transform(X_train)\n",
    "        X_test_scaled = self.scalers['standard'].transform(X_test)\n",
    "\n",
    "        # Train multiple models\n",
    "        models_config = {\n",
    "            'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'SVC': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "        }\n",
    "\n",
    "        best_model_name = None\n",
    "        best_score = 0.0\n",
    "\n",
    "        for model_name, model in models_config.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "                print(f\"{model_name} CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "                \n",
    "                # Train on full training set\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Test evaluation\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
    "                print(classification_report(y_test, y_pred, zero_division=0))\n",
    "                \n",
    "                self.models[model_name] = model\n",
    "                self.model.training_history['model_scores'][model_name] = accuracy\n",
    "                self.model.training_history['cv_scores'][model_name] = cv_scores.mean()\n",
    "                \n",
    "                if accuracy > best_score:\n",
    "                    best_score = accuracy\n",
    "                    best_model_name = model_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name}: {str(e)}\")\n",
    "\n",
    "        # Create ensemble model\n",
    "        if len(self.models) >= 2:\n",
    "            print(\"\\nCreating ensemble model...\")\n",
    "            ensemble = VotingClassifier(\n",
    "                estimators=[(name, model) for name, model in self.models.items()],\n",
    "                voting='soft'\n",
    "            )\n",
    "            ensemble.fit(X_train_scaled, y_train)\n",
    "            y_pred_ensemble = ensemble.predict(X_test_scaled)\n",
    "            ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "            \n",
    "            print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(classification_report(y_test, y_pred_ensemble, zero_division=0))\n",
    "            \n",
    "            self.models['Ensemble'] = ensemble\n",
    "            self.model.training_history['model_scores']['Ensemble'] = ensemble_accuracy\n",
    "            \n",
    "            if ensemble_accuracy > best_score:\n",
    "                best_score = ensemble_accuracy\n",
    "                best_model_name = 'Ensemble'\n",
    "\n",
    "        self.model.best_model = self.models.get(best_model_name)\n",
    "        self.model.best_score = best_score\n",
    "        \n",
    "        print(f\"\\n=== Best Model: {best_model_name} with accuracy {best_score:.4f} ===\")\n",
    "        \n",
    "        # Save models\n",
    "        with open('trained_models.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'models': self.models,\n",
    "                'scalers': self.scalers,\n",
    "                'feature_cols': feature_cols,\n",
    "                'best_model': best_model_name\n",
    "            }, f)\n",
    "        \n",
    "        print(\"Models saved to trained_models.pkl\")\n",
    "        self.memory_manager.force_cleanup()\n",
    "\n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all videos\"\"\"\n",
    "        if not self.models or self.model.best_model is None:\n",
    "            print(\"ERROR: No trained models available. Please train models first.\")\n",
    "            return\n",
    "\n",
    "        print(\"Generating predictions...\")\n",
    "\n",
    "        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in numeric_cols if col not in ['video_id', 'incident_detection', 'incident_start_frame']]\n",
    "        \n",
    "        X = self.processed_data[feature_cols].fillna(0)\n",
    "        X_scaled = self.scalers['standard'].transform(X)\n",
    "\n",
    "        predictions = self.model.best_model.predict(X_scaled)\n",
    "        probabilities = self.model.best_model.predict_proba(X_scaled)[:, 1] if hasattr(self.model.best_model, 'predict_proba') else predictions\n",
    "\n",
    "        self.predictions_df = self.processed_data.copy()\n",
    "        self.predictions_df['predicted_incident'] = predictions\n",
    "        self.predictions_df['incident_probability'] = probabilities\n",
    "\n",
    "        print(f\"Predictions generated for {len(self.predictions_df)} videos\")\n",
    "        print(f\"Predicted incidents: {sum(predictions)} ({sum(predictions)/len(predictions)*100:.2f}%)\")\n",
    "\n",
    "        return self.predictions_df\n",
    "\n",
    "    def evaluate_text_quality(self, sample_size=100):\n",
    "        \"\"\"Evaluate text generation quality using METEOR, CiDER-D, and SPICE\"\"\"\n",
    "        if self.processed_data is None:\n",
    "            print(\"ERROR: No processed data available\")\n",
    "            return\n",
    "\n",
    "        print(\"Evaluating text generation quality...\")\n",
    "\n",
    "        sample_data = self.processed_data.sample(min(sample_size, len(self.processed_data)))\n",
    "\n",
    "        meteor_scores = []\n",
    "        cider_scores = []\n",
    "        spice_scores = []\n",
    "\n",
    "        for idx, row in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Evaluating text\"):\n",
    "            # Generate reference text\n",
    "            reference_caption = f\"The vehicle is driving on a road during normal conditions.\"\n",
    "            reference_reason = f\"Incident occurred due to {row['label']}\"\n",
    "\n",
    "            # Calculate metrics for caption\n",
    "            meteor_caption = self.text_metrics.calculate_meteor(reference_caption, row['caption_before'])\n",
    "            cider_caption = self.text_metrics.calculate_cider_d(reference_caption, row['caption_before'])\n",
    "            spice_caption = self.text_metrics.calculate_spice(reference_caption, row['caption_before'])\n",
    "\n",
    "            # Calculate metrics for reason\n",
    "            meteor_reason = self.text_metrics.calculate_meteor(reference_reason, row['reason'])\n",
    "            cider_reason = self.text_metrics.calculate_cider_d(reference_reason, row['reason'])\n",
    "            spice_reason = self.text_metrics.calculate_spice(reference_reason, row['reason'])\n",
    "\n",
    "            meteor_scores.append((meteor_caption + meteor_reason) / 2)\n",
    "            cider_scores.append((cider_caption + cider_reason) / 2)\n",
    "            spice_scores.append((spice_caption + spice_reason) / 2)\n",
    "\n",
    "        self.all_metrics = {\n",
    "            'METEOR': np.mean(meteor_scores),\n",
    "            'CiDER-D': np.mean(cider_scores),\n",
    "            'SPICE': np.mean(spice_scores)\n",
    "        }\n",
    "\n",
    "        print(\"\\n=== Text Generation Quality Metrics ===\")\n",
    "        for metric, score in self.all_metrics.items():\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "        return self.all_metrics\n",
    "\n",
    "    def generate_submission_file(self, output_path=\"submission.json\"):\n",
    "        \"\"\"Generate final submission file in required format\"\"\"\n",
    "        if self.predictions_df is None:\n",
    "            print(\"ERROR: No predictions available. Please run generate_predictions() first.\")\n",
    "            return\n",
    "\n",
    "        print(\"Generating submission file...\")\n",
    "\n",
    "        submission = {}\n",
    "\n",
    "        for idx, row in tqdm(self.predictions_df.iterrows(), total=len(self.predictions_df), desc=\"Creating submission\"):\n",
    "            video_id = str(int(row['video_id']))\n",
    "\n",
    "            submission[video_id] = {\n",
    "                \"incident_start_frame\": int(row['incident_start_frame']),\n",
    "                \"incident_detection\": int(row['predicted_incident']),\n",
    "                \"severity\": row['severity'],\n",
    "                \"ego_involved\": int(row['ego_involved']),\n",
    "                \"label\": row['label'],\n",
    "                \"num_bicyclists\": int(row['num_bicyclists']),\n",
    "                \"num_animals\": int(row['num_animals']),\n",
    "                \"num_pedestrians\": int(row['num_pedestrians']),\n",
    "                \"num_vehicles\": int(row['num_vehicles']),\n",
    "                \"caption_before\": row['caption_before'],\n",
    "                \"reason\": row['reason']\n",
    "            }\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(submission, f, indent=2)\n",
    "\n",
    "        print(f\"Submission file saved to {output_path}\")\n",
    "        print(f\"Total predictions: {len(submission)}\")\n",
    "\n",
    "        return submission\n",
    "\n",
    "    def run_full_pipeline(self, max_videos=100):\n",
    "        \"\"\"Execute complete pipeline from feature extraction to submission\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"Starting 2COOOL Pipeline\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Step 1: Extract video features\n",
    "        self.extract_all_features(max_videos=max_videos)\n",
    "\n",
    "        # Step 2: Prepare ML dataset\n",
    "        self.prepare_ml_dataset()\n",
    "\n",
    "        # Step 3: Perform EDA\n",
    "        self.perform_eda()\n",
    "\n",
    "        # Step 4: Train models\n",
    "        self.train_models()\n",
    "\n",
    "        # Step 5: Generate predictions\n",
    "        self.generate_predictions()\n",
    "\n",
    "        # Step 6: Evaluate text quality\n",
    "        self.evaluate_text_quality()\n",
    "\n",
    "        # Step 7: Generate submission\n",
    "        self.generate_submission_file()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Print final summary\n",
    "        print(\"\\n=== Final Summary ===\")\n",
    "        print(f\"Videos processed: {len(self.video_features)}\")\n",
    "        print(f\"Best model: {list(self.models.keys())[0] if self.models else 'None'}\")\n",
    "        print(f\"Best accuracy: {self.model.best_score:.4f}\")\n",
    "        print(f\"Text quality metrics: {self.all_metrics}\")\n",
    "        print(f\"Submission file: submission.json\")\n",
    "\n",
    "        self.memory_manager.force_cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = COOOLPipeline(root_path=\"/kaggle/input/d-drive-2cool-competition-video-data-final\")\n",
    "\n",
    "    # Run complete pipeline\n",
    "    pipeline.run_full_pipeline(max_videos=100)\n",
    "\n",
    "    # Optional: Save processed data for later use\n",
    "    if pipeline.processed_data is not None:\n",
    "        pipeline.processed_data.to_csv('submission.csv', index=False)\n",
    "        print(\"Processed data saved to submission.csv\")\n",
    "\n",
    "    print(\"\\n=== 2COOOL Pipeline Execution Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cff01",
   "metadata": {
    "papermill": {
     "duration": 0.004859,
     "end_time": "2025-10-02T05:44:06.326063",
     "exception": false,
     "start_time": "2025-10-02T05:44:06.321204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13485693,
     "sourceId": 112002,
     "sourceType": "competition"
    },
    {
     "datasetId": 8175278,
     "sourceId": 12920093,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 275.550739,
   "end_time": "2025-10-02T05:44:08.952147",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-02T05:39:33.401408",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0613993857e148928dbe1df0d8a35527": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0957107dd7af4c3e886130fa9463df10",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_d36b38e9eb5048a19340bd3674e3ea28",
       "tabbable": null,
       "tooltip": null,
       "value": "Extractingâ€‡videoâ€‡features:â€‡100%"
      }
     },
     "063604fb2f714035981e0c1bbfc55129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06b196f076194c1fb1a021f241c02bd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0957107dd7af4c3e886130fa9463df10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b89e54027a847ecb265d8d047ea7cbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f7ff5f10b814b50bb10228d02262882": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c924725a9db348ae868dd599156e71c7",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_41024a1bea0849c098ed939d583242af",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "10f0b44255cf4a66a6a570d7d680c75a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b60684699254fb9aa0c5dd026982b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca8a8b70dff49d9b5ac2a617e0a8346": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d6cdd55e3874f04a0770891d28c3542": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0613993857e148928dbe1df0d8a35527",
        "IPY_MODEL_0f7ff5f10b814b50bb10228d02262882",
        "IPY_MODEL_803d18465eeb45e78192f26ce6fc20df"
       ],
       "layout": "IPY_MODEL_e7b6b0fd49ca48628129baf40c7ef0c5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "41024a1bea0849c098ed939d583242af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4e6aa5037cea480fbec5abe2e3355be4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "575b8fe5302f46b98e137ee2a6f4ae50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59c492ac45b14bc58fa615952c122d95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_631bd6ba64ad4fdf8309ff953803c658",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c6b0b389126e4803a070f84998ccef04",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡100/100â€‡[00:00&lt;00:00,â€‡4360.39it/s]"
      }
     },
     "631bd6ba64ad4fdf8309ff953803c658": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dd175f51325425388b9c8becfbe5070": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fa31aab6dc34a65b74ede01636bfea4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4e6aa5037cea480fbec5abe2e3355be4",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e5b3c7308b274818bb06f428887967f1",
       "tabbable": null,
       "tooltip": null,
       "value": "Creatingâ€‡submission:â€‡100%"
      }
     },
     "803d18465eeb45e78192f26ce6fc20df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f123dacb278642d98274dfe473bff4e8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0b89e54027a847ecb265d8d047ea7cbb",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡100/100â€‡[03:23&lt;00:00,â€‡â€‡2.04s/it,â€‡Memory=1.29GB,â€‡Features=100]"
      }
     },
     "808771084ff04473ba98657ff150c16f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d653370a636d40c7a587a59031b77122",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bbd343d6fb70478d8c6e5cd4bc062fa0",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡100/100â€‡[00:00&lt;00:00,â€‡1662.51it/s]"
      }
     },
     "82ce59d2ac4e426c9138f875ed75a7b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4b0dc087185462e8892b7895d4dec9e",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e9f3405f0f3441e685e476c4de075e54",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "842a103bb71d43b7a1cfdd26e92c934f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aa533eb14e8e473fb1b4286070d8ea1b",
        "IPY_MODEL_d66eb4dae43f45b7af84f2ad9a7e947d",
        "IPY_MODEL_998a3bf568a94a91826987727a994ee5"
       ],
       "layout": "IPY_MODEL_10f0b44255cf4a66a6a570d7d680c75a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "84448650b3d54c6eb14379a247d4b3ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "89fd895a99804013b4ba2d31584fe356": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cc38f6730871476495eb6a8260a3fcc3",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a189774963c14ae9b5d6cba8910fab4e",
       "tabbable": null,
       "tooltip": null,
       "value": "Preparingâ€‡dataset:â€‡100%"
      }
     },
     "998a3bf568a94a91826987727a994ee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6dd175f51325425388b9c8becfbe5070",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_575b8fe5302f46b98e137ee2a6f4ae50",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡100/100â€‡[00:06&lt;00:00,â€‡38.10it/s]"
      }
     },
     "a189774963c14ae9b5d6cba8910fab4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a82bc1a162a146b98a08a50e038acc96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_89fd895a99804013b4ba2d31584fe356",
        "IPY_MODEL_a88f2db6e3b54b11ac8cc80014f07d7b",
        "IPY_MODEL_808771084ff04473ba98657ff150c16f"
       ],
       "layout": "IPY_MODEL_063604fb2f714035981e0c1bbfc55129",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a88f2db6e3b54b11ac8cc80014f07d7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b60684699254fb9aa0c5dd026982b95",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fc11f253be64444e8f9917b6fe496cb2",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "aa533eb14e8e473fb1b4286070d8ea1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ca8a8b70dff49d9b5ac2a617e0a8346",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_84448650b3d54c6eb14379a247d4b3ae",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluatingâ€‡text:â€‡100%"
      }
     },
     "af540ffd842b4aca8a4a3013ad24ff09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6fa31aab6dc34a65b74ede01636bfea4",
        "IPY_MODEL_82ce59d2ac4e426c9138f875ed75a7b1",
        "IPY_MODEL_59c492ac45b14bc58fa615952c122d95"
       ],
       "layout": "IPY_MODEL_d9fd8b01fbf446cb9ce07dbc9777bcdc",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b4b0dc087185462e8892b7895d4dec9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbd343d6fb70478d8c6e5cd4bc062fa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c6b0b389126e4803a070f84998ccef04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c924725a9db348ae868dd599156e71c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc38f6730871476495eb6a8260a3fcc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d36b38e9eb5048a19340bd3674e3ea28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d653370a636d40c7a587a59031b77122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d66eb4dae43f45b7af84f2ad9a7e947d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f6f316496a384637b463339e3e55457f",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_06b196f076194c1fb1a021f241c02bd2",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "d9fd8b01fbf446cb9ce07dbc9777bcdc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5b3c7308b274818bb06f428887967f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e7b6b0fd49ca48628129baf40c7ef0c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9f3405f0f3441e685e476c4de075e54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f123dacb278642d98274dfe473bff4e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6f316496a384637b463339e3e55457f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc11f253be64444e8f9917b6fe496cb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
